{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67fe434a-7306-4aed-8812-dce72b1e7623",
   "metadata": {},
   "source": [
    "# Mobile Robotics : Group T25 Report Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d97859-8637-4c4c-94d3-8b7860f18443",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "\n",
    "* [1. Introduction](#introduction)\n",
    "  \n",
    "* [2. Computer Vision](#cv)\n",
    "    * [2.1. Introduction to Yolov8](#yolov8)  \n",
    "    * [2.2. Application on the Thymio](#application_yolo)\n",
    "        * [2.2.1. Images of System Detection](#cv_images) \n",
    "    * [2.3. Detection function](#detection_function)\n",
    "      * [2.3.1. Packages required](#CV_packages)\n",
    "      * [2.3.2. Function explanation](#CV_explanation)\n",
    "    * [2.4. Annotation function](#annotation)  \n",
    "      \n",
    "* [3. Global Path](#global_path)\n",
    "    * [3.1. Overall Path strategy](#path_strategy)\n",
    "    * [3.2. Global Path Planning](#global_path_planning)\n",
    "    * [3.3. Global Path Function](#global_path_function)\n",
    "        * [3.3.1. Packages required ](#global_path_packages)\n",
    "        * [3.3.2. Function Explanation ](#global_path_explanation)\n",
    "        * [3.3.3. Example of global path ](#global_path_example)\n",
    "  \n",
    "* [4. Local Navigation](#local_navigation)\n",
    "    * [4.1.Local Navigation Function](#local_navigation_function)\n",
    "    * [4.2 Key parameters](#key_parameters)\n",
    "      \n",
    "* [5. Motion Control](#motion_control)\n",
    "    * [5.1. Drive function](#drive)\n",
    "    * [5.2. Turn function](#turn)  \n",
    "* [6. Filtering](#filtering)\n",
    "\n",
    "  \n",
    "* [7. Main code](#main_code)\n",
    "    * [7.1. Packages and Functions importation](#packages_functions_importation)\n",
    "    * [7.2.Initialize Thymio functions](#Thymio_functions)\n",
    "    * [7.3.Main loop](#main_loop)\n",
    "    \n",
    "* [8. Videos demonstration](#videos_demonstration)\n",
    "    * [8.1. Demonstration without kidnapping](#videos_demonstration_without)\n",
    "    * [8.2. Demonstration with kidnapping](#videos_demonstration_with)\n",
    " \n",
    "* [8. Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb319468-62b5-4f4a-9687-613431564804",
   "metadata": {},
   "source": [
    "## 1. Introduction <a class=\"anchor\" id=\"introduction\"></a>\n",
    "\n",
    "The aim of this project was to integrate the different themes currently being seen in one project: path planning, vision, local navigation and filtering. To combine these, we treated our robot as a spaceship.\n",
    "\n",
    "Thymio Mars V aims to reach Mars in the shortest possible time, avoiding black holes along the way. To do this, the camera detects the robot's position, the position of Mars and the position of the black holes. To determine the shortest route, we decided to use the A* algorithm, as it's easy to implement and doesn't require a lot of resources. If Thymio Mars V encounters an asteroid field on its path, obstacle detection takes control and, thanks to the proximity sensors, the robot is able to avoid it.\n",
    "\n",
    "Finally, Markov Filtering is used to be sure that the robot starts at the right position because this type of filtering is effective when there is uncertainty about the position and speed, which is the case in this project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27b611c-196c-4a7c-89dc-4cdbe72e11ef",
   "metadata": {},
   "source": [
    "## 2. Computer Vision <a class=\"anchor\" id=\"cv\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e339fe-0a6c-4b0a-b1aa-4243cf754963",
   "metadata": {},
   "source": [
    "### 2.1. Introduction to Yolov8 <a class=\"anchor\" id=\"yolov8\"></a>\n",
    "\n",
    "YOLOv8 is the newest state-of-the-art YOLO model that can be used for object detection, image classification, and instance segmentation tasks. YOLOv8 was developed by Ultralytics, who also created the influential and industry-defining YOLOv5 model. YOLOv8 includes numerous architectural and developer experience changes and improvements over YOLOv5.\n",
    "\n",
    "Yolov8 is based on a Convolutional neural network, the main idea is to train a model using this CNN with an adapted dataset.\n",
    "\n",
    "\n",
    "Convolutional neural network (CNN) is a regularized type of feed-forward neural network that learns feature engineering by itself via filters (or kernel) optimization. Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by using regularized weights over fewer connections.For example, for each neuron in the fully-connected layer 10,000 weights would be required for processing an image sized 100 × 100 pixels.\n",
    "\n",
    "Sources :\n",
    "\n",
    "https://blog.roboflow.com/whats-new-in-yolov8/\n",
    "\n",
    "https://en.wikipedia.org/wiki/Convolutional_neural_network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed401ef-8bcf-4cb4-99a4-893cee2d5886",
   "metadata": {},
   "source": [
    "### 2.2. Application of Yolov8 on the thymio <a class=\"anchor\" id=\"application_yolo\"></a>\n",
    "\n",
    "\n",
    "The basic idea was to use yolov8 to detect the robot, start point and obstacles at any point in time. We first train our model with :\n",
    "- annotation of a dataset on Robotflow (https://app.roboflow.com/gabriel-paffi)\n",
    "- training our system on a google colab to accelare our training process (https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb#scrollTo=I4bpUIibcV1l)\n",
    "\n",
    "Once we have obtained our model trained on our dataset, we can use this model to predict the class, position and confidence of any object of our datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e384d7e-fbaf-40b4-963f-b87df19e006a",
   "metadata": {},
   "source": [
    "#### 2.2.1 Images of System Detection <a class=\"anchor\" id=\"cv_images\"></a>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./cv_images/robot.png\" alt=\"System detection objects\">\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./cv_images/System.png\" alt=\"System detection objects\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94abc7f8-3508-4c16-b06b-d447f37987c0",
   "metadata": {},
   "source": [
    "### 2.3. Detection function <a class=\"anchor\" id=\"detection_function\"></a>\n",
    "\n",
    "\n",
    "### 2.3.1. Packages required <a class=\"anchor\" id=\"CV_packages\"></a>\n",
    "\n",
    "- open cv2 : https://opencv.org/\n",
    "- Yolov8 from ultralytics : https://github.com/ultralytics/ultralytics\n",
    "\n",
    "### 2.3.2. Explanation of the function <a class=\"anchor\" id=\"CV_explanation\"></a>\n",
    "\n",
    "This function take into input a frame and return :\n",
    "- an array containing the different object as follow : (class, x_center, y_center, width_object, height_object)\n",
    "- a frame annotated with the class object on it and the confidence accord to the object\n",
    "\n",
    "This function use our train model (robotv12.pt) and a minimum confidence to detect an object of 0.55 (1 is maximum confidence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c822e7f8-83bf-4a78-a395-6189cd67c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "model = YOLO(\"robotv12.pt\")\n",
    "class_object = [\"blackholes\",\"earth\",\"mars\", \"robot\" ]\n",
    "\n",
    "def detection(frame):\n",
    "\n",
    "    confidence_min = 0.55\n",
    "\n",
    "    results = model.track(frame, conf = confidence_min, verbose = False)\n",
    "\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "\n",
    "    array_box_dim_tot = []\n",
    "           \n",
    "   \n",
    "\n",
    "    for objects in results:\n",
    "    \n",
    "            number_objects = len(objects.boxes.cls)\n",
    "            box_dimensions = objects.boxes.xywhn\n",
    "            for i in range(number_objects) :\n",
    "              \n",
    "                array_box_dim_object = []\n",
    "                confidence = round(objects.boxes.conf[i].item(),3)\n",
    "                cls = objects.boxes.cls[i].item()\n",
    "                array_box_dim = box_dimensions[i].tolist() # center x,y width, height\n",
    "                for i in range(len(array_box_dim)):\n",
    "                    array_box_dim[i] = round(array_box_dim[i],2)\n",
    "                array_box_dim_object = [class_object[int(cls)], array_box_dim, confidence]\n",
    "\n",
    "                array_box_dim_tot.append(array_box_dim_object)\n",
    "\n",
    "\n",
    "                        \n",
    "\n",
    "    return(array_box_dim_tot, annotated_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092ff158-a50c-4ae9-880b-b2a477d2f159",
   "metadata": {},
   "source": [
    "### 2.4. Annotation function <a class=\"anchor\" id=\"annotation\"></a>\n",
    "\n",
    "This function goal is to add the path of the robot on the frame.\n",
    "The input of this function are :\n",
    "- a frame\n",
    "- coordinates : the coordinates of each point from the robot towards the goal\n",
    "\n",
    "The output is the frame with segment between each point of the path.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "760dbfc4-7c48-4bbd-bba0-671268ddf2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation(frame, coord):\n",
    "    \n",
    "    for k in range(len(coord)-1):\n",
    "        \n",
    "        frame = cv2.circle(frame, coord[k], circle_radius, blue,circle_thicknes)\n",
    "        frame = cv2.line(frame, coord[k], coord[k+1], blue, line_thickness)  \n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25b4e80-c817-4a70-b80b-5cc8a162755b",
   "metadata": {},
   "source": [
    "## 3. Global Path <a class=\"anchor\" id=\"global_path\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7badd0-e805-4c63-b463-4bd90d6a6524",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1. Overall Path Strategy <a class=\"anchor\" id=\"path strategy\"></a>\n",
    "\n",
    "Space is a hostile place where a lot of unpredictable things can happen. We therefore wanted a robust system that is resilient to changes in object placements, and changes to camera placement. A system that updates these position at each timestep will be resilient to any changes. However, computing object placements and computing the global path takes time. We therefore chose to move the robot in steps of ~5cm after which we recompute positions and global path. This has the drawback of our robot stopping every ~5cm. The advantage is that any new objects will be detected quickly, and if the goal is moved, the robot will quickly adjust its path. It also has the advantage that kidnapping is very easy to sort out.\n",
    "\n",
    "If we do not get any input from the camera, we rely on the last calculated global path and the position and angle estimated by the robot itself. \n",
    "\n",
    "The angle of the robot is calculated by both the camera and the robot. For each step the robot turns a certain amount and from this it knows its angle. The camera can also calculate the angle of the tracetory of the robot when looking at two timesteps, and from this calculate the angle. The angle used by the system is the average of these two calculated angles. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086c97c2-232e-451e-a1dd-74049f0ca74c",
   "metadata": {},
   "source": [
    "### 3.2. Global Path Planning <a class=\"anchor\" id=\"global_path_planning\"></a>\n",
    "\n",
    "To succesfully navigate an environment a path needs to be computed. For our 'space' environment we wanted a path close to the optimal path and a efficient computation. Visibility graphs are very efficient and can find the optimal path. However, with round planets this does not work because of the missing edges. While the planets could be approximated with polygons to be used with the visibility graph, we did not chose this for our project \n",
    "\n",
    "The A* algorithm is optimal and relatively efficient to compute which is why we chose this method. The A* algorithm works on a grid with discrete points and therefore has a tradeoff between resolution and computation time. For the map a resolution of 100x70 was chosen since this is the same aspect ratio as the camera used. This offers a computation time of around 70ms for the gloabl path as seen below. Since A* operates with discrete points the path is not necessarily straight between two points. The A* algorithm implemented here uses 16 directions as the standard, which results in the blue path shown in the figure below. As mentioned before this path is not straight between point. To solve this the path is modified by a line of sight algorithm. This algorithm draws a line to the furthest possible point and removes all intermediate points. This process is repeated from the next point. Since our robot operates from point to point a too long path, the long segments are divided into subsegment with a max length of 4. This results in the red path. This path has a lot less points and goes in a straight line between points. \n",
    "\n",
    "As seen in the figure below, the optimal path goes all the way to the edge of the black parts of the map. This is because the radius of the robot is added to the radius of the black holes in the map, when driving the robot along the path we thereby avoid hitting the black holes.\n",
    "\n",
    "Sources : \n",
    "https://en.wikipedia.org/wiki/A*_search_algorithm \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14e9f5c-d648-4053-90ef-2e8f0e1c470d",
   "metadata": {},
   "source": [
    "### 3.3. Global path function <a class=\"anchor\" id=\"global_path_function\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c307ea-75ed-4497-a8a3-83830d713a62",
   "metadata": {},
   "source": [
    "### 3.3.1. Packages required<a class=\"anchor\" id=\"global_path_packages\"></a>\n",
    "\n",
    "\n",
    "- pandas\n",
    "- matplotlib\n",
    "- time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9901d58-4fe0-4c68-9609-475e481bad65",
   "metadata": {},
   "source": [
    "### 3.3.2. Functions explanations <a class=\"anchor\" id=\"global_path_explanation\"></a>\n",
    "\n",
    "This function take into input :\n",
    "- start : a list with x and y positions of the robot\n",
    "-  goal : a list with x and y positions of mars\n",
    "-  black_holes_centers :  a list with x and y positions of blackholes\n",
    "-  black_holes_radius :  a  a list with radius of blackholes\n",
    "   \n",
    "and return as output :\n",
    "- a path : array of n from robot to mars coordinates x,y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d17778-1210-495b-90a7-487ddccd83b1",
   "metadata": {},
   "source": [
    " ### 3.3.3 Example of global path <a class=\"anchor\" id=\"global_path_example\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a460f469-936e-442a-ba81-0bdce01ee202",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Global_nav' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m global_nav \u001b[38;5;241m=\u001b[39m \u001b[43mGlobal_nav\u001b[49m(movements\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m8N\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m coordinates \u001b[38;5;241m=\u001b[39m  [[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrobot\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.95\u001b[39m, \u001b[38;5;241m0.09\u001b[39m, \u001b[38;5;241m0.13\u001b[39m], \u001b[38;5;241m0.965\u001b[39m], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmars\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;241m0.95\u001b[39m, \u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.08\u001b[39m, \u001b[38;5;241m0.08\u001b[39m], \u001b[38;5;241m0.902\u001b[39m], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearth\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.95\u001b[39m, \u001b[38;5;241m0.08\u001b[39m, \u001b[38;5;241m0.1\u001b[39m], \u001b[38;5;241m0.942\u001b[39m], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblackholes\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.85\u001b[39m, \u001b[38;5;241m0.11\u001b[39m, \u001b[38;5;241m0.15\u001b[39m], \u001b[38;5;241m0.906\u001b[39m], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblackholes\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.45\u001b[39m, \u001b[38;5;241m0.09\u001b[39m, \u001b[38;5;241m0.09\u001b[39m], \u001b[38;5;241m0.896\u001b[39m], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblackholes\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;241m0.65\u001b[39m, \u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.06\u001b[39m, \u001b[38;5;241m0.08\u001b[39m], \u001b[38;5;241m0.876\u001b[39m], [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblackholes\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;241m0.07\u001b[39m, \u001b[38;5;241m0.08\u001b[39m], \u001b[38;5;241m0.867\u001b[39m]]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Global_nav' is not defined"
     ]
    }
   ],
   "source": [
    "global_nav = Global_nav(movements='8N')\n",
    "coordinates =  [['robot', [0.05, 0.95, 0.09, 0.13], 0.965], ['mars', [0.95, 0.05, 0.08, 0.08], 0.902], ['earth', [0.05, 0.95, 0.08, 0.1], 0.942], ['blackholes', [0.2, 0.85, 0.11, 0.15], 0.906], ['blackholes', [0.25, 0.45, 0.09, 0.09], 0.896], ['blackholes', [0.65, 0.25, 0.06, 0.08], 0.876], ['blackholes', [0.8, 0.4, 0.07, 0.08], 0.867]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7dd38d11-bbba-4be4-b843-bf385dedc145",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'global_nav' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'global_nav' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start, goal, earth, robot_radius, earth_radius, black_holes_centers, black_holes_radiuss,scale_factor = global_nav.convert_OPENCV_tovalues(coordinates)\n",
    "occ, _,_=  global_nav.create_map( goal = goal ,black_holes_centers = black_holes_centers,black_holes_radiuss=black_holes_radiuss)\n",
    "opti_path, path, closed, open = global_nav.get_path_straight(start=start,goal=goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b60bea57-ba3b-4b85-8aa3-bd818a5f676f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m7.5\u001b[39m))\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(occ\u001b[38;5;241m.\u001b[39mtranspose(), cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGreys\u001b[39m\u001b[38;5;124m'\u001b[39m, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter([x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m closed], [x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m closed], c\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrey\u001b[39m\u001b[38;5;124m'\u001b[39m,s\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(10,7.5))\n",
    "plt.imshow(occ.transpose(), cmap='Greys', origin='lower')\n",
    "plt.scatter([x[0] for x in closed], [x[1] for x in closed], c='grey',s=5, alpha=0.3)\n",
    "plt.scatter([x[0] for x in black_holes_centers], [x[1] for x in black_holes_centers], c='grey',s=100)\n",
    "plt.scatter(earth[0],earth[1], c='green',s=300)\n",
    "plt.scatter(goal[0],goal[1], c='orange',s=300)\n",
    "plt.scatter([x[0] for x in path], [x[1] for x in path],color = 'b',s=5)\n",
    "plt.scatter([x[0] for x in opti_path], [x[1] for x in opti_path], c='r',s=15)\n",
    "plt.plot([x[0] for x in opti_path], [x[1] for x in opti_path], c='r', alpha=0.5)\n",
    "plt.legend(['Investigated nodes','Black holes centers', 'Earth', 'Mars' , 'A* path', 'Optimized path'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c785a991-23a3-43f3-a775-39b235a3f4a4",
   "metadata": {},
   "source": [
    "## 4. Local Navigation <a class=\"anchor\" id=\"local_navigation\"></a>\n",
    "\n",
    "Thymio needs to avoid obstacles, in this case asteroids, as it travels through space. To do this, we call the function local_avoidance() that allows Thymio to avoid the obstacle and continue it way to Mars.\n",
    "\n",
    "Sources:\n",
    "\n",
    "Solutions Week 3 - Artificial neural networks.ipynb\n",
    "\n",
    "Some structure of the jupyter notebooks has been borrowed by the GitHub Repo https://github.com/hibetterheyj/Thymio_Vision_Guided_Navigation/blob/master/BMR_Final_Project.ipynb \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f1543d-0b6f-41ae-845a-ed40daad595a",
   "metadata": {},
   "source": [
    "### 4.1 Local Navigation Function <a class=\"anchor\" id=\"local_navigation_function\"></a>\n",
    "\n",
    "#### Input:\n",
    "- Current motors speed, horizotnal proximity sensors values\n",
    "\n",
    "This function will take the current value of the motors and, depending on the weights and values of the various sensors, adjust the values of the motors so that it dodges the obstacle.\n",
    "\n",
    "#### Output:\n",
    "- new motors values (new motor_left_target,motor_right_target) allowing to avoid the obstacle\n",
    "- To avoid the robot to be blocked when it faces a front obstacle we adjust the weight so that it has a higher tendency to go on left.\n",
    "\n",
    "#### Call:\n",
    "- At each step of global navigation\n",
    "\n",
    "In reality, when the global path function is called, it generates a path with segments. It then calls the drive() function, which splits the segment into 10 smaller segments. This is where we call the get_prox() function to compare the value of the proximity sensors with our WALL_THRESHOLD. This method gives the robot better detection on its path between two points on the layout.\n",
    "\n",
    "#### Limitations:\n",
    "The first limitation of this implementation lies in the call. Dividing the distance between 2 points into smaller distances partially solves the problem, because if an obstacle appears between these two smaller distances, the previous problem reappears. However, we don't do this when we're turning, which may mean that the robot doesn't detect obstacles properly when repositioning itself. \n",
    "A better approach would have been to use two different threads in parallel. Because what can happen is that the robot oscillates between the global_path() and local_navigation() functions and jerks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f489b8ef-f42e-4e67-a39a-95929d12810f",
   "metadata": {},
   "source": [
    "### 4.2 Keys parameters <a class=\"anchor\" id=\"key_paramters\"></a>\n",
    "\n",
    "| Name | Meaning | Type (Unit) | Global|\n",
    "|-----------|-----------|-----------|-----------|\n",
    "|max_allowed_speed|Limit the speed outpout of the motor in local_navigation|int|no|\n",
    "|mem_sensor|Store the values of the 5 front sensors get by get_prox()|int|no|\n",
    "|sensor_scale|Rescale the output value of the sensors|int|no|\n",
    "|WALL_THRESHOLD|Threshold to determined when there is an obsctacle|int|yes||\n",
    "|ROBOT_SPEED|Current robot speed|int|yes|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1e95e8-19aa-4a23-8cee-0f199e702ff5",
   "metadata": {},
   "source": [
    "## 5. Motion control <a class=\"anchor\" id=\"motion_control\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0494609f-4319-47be-8630-250dcbb401a0",
   "metadata": {},
   "source": [
    "### 5.1. Drive functions <a class=\"anchor\" id=\"drive\"></a>\n",
    "\n",
    "The drive function works by sleeping for the amount of s required to drive a distance of X [cm]. We started off by calibrating the robot: the Thymio used for our project drives 6cm in 1s. This gives us a speed_drive = 6 cm/s for a motor speed of 150. The time the robot needs to sleep to drive a distance is: dist/speed_drive = [s]\n",
    "\n",
    "The driving is split into two functions: one that simply sets the speed of the motors to the desired speed, and another one that sets calculates the time the robot has to drive and makes it drive for that amount of seconds using time.sleep() [s]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41aa6d62-a3df-46b7-8f34-3992759a1124",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tdmclient.notebook.sync_var\n",
    "# drive straight with \"speed\" (motor speed)\n",
    "def motors_drive(l_speed=500, r_speed=500):\n",
    "    global motor_left_target, motor_right_target\n",
    "    \n",
    "    motor_left_target = l_speed\n",
    "    motor_right_target = r_speed\n",
    "\n",
    "# drive \"dist\" [cm]\n",
    "def drive(dist=10):\n",
    "    global WALL_THRESHOLD, ROBOT_SPEED\n",
    "    motors_drive(ROBOT_SPEED,ROBOT_SPEED) #test with lower speed value\n",
    "    for i in range(10):\n",
    "        time.sleep((dist/(2*3))/10) \n",
    "        if max(get_prox()) > WALL_THRESHOLD:\n",
    "            print(\"obstacle detected\")\n",
    "            local_navigation()\n",
    "            break\n",
    "    motors_drive(0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f075b92a-5a9b-40cc-80ef-d4d1fc33df55",
   "metadata": {},
   "source": [
    "### 5.2. Turn functions <a class=\"anchor\" id=\"turn\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2cd297-d981-4c11-be3b-04f56a86f7f7",
   "metadata": {},
   "source": [
    "## 6. Filtering <a class=\"anchor\" id=\"filtering\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92bc15b-4b6b-43ff-9fb5-c057bad1e63d",
   "metadata": {},
   "source": [
    "## 7. Main code <a class=\"anchor\" id=\"main_code\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd9f2b5-ec1b-4140-af59-7f4ef787214c",
   "metadata": {},
   "source": [
    "### 7.1. Packages and Functions importation <a class=\"anchor\" id=\"packages_functions_importation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45136a5d-1786-4561-b621-e99a7c3de0ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'CV_function'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mCV_function\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpath_object\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mMarkov_Filter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'CV_function'"
     ]
    }
   ],
   "source": [
    "import math \n",
    "import numpy as np\n",
    "import tdmclient.notebook\n",
    "\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from CV_function import *\n",
    "from path_object import *\n",
    "from Markov_Filter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e995e8c-5f94-46e2-9950-477f2973d38c",
   "metadata": {},
   "source": [
    "### 7.2. Initialize Thymio functions <a class=\"anchor\" id=\"Thymio_functions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "29b0317e-9e83-4883-95a6-16f4424a5bce",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m tdmclient\u001b[38;5;241m.\u001b[39mnotebook\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m      7\u001b[0m WALL_THRESHOLD \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m\n\u001b[1;32m      8\u001b[0m ROBOT_SPEED \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m150\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tdmclient/notebook/private.py:107\u001b[0m, in \u001b[0;36mstart\u001b[0;34m(zeroconf, zeroconf_all, tdm_ws, tdm_addr, tdm_port, password, debug, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart\u001b[39m(zeroconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, zeroconf_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     91\u001b[0m                 tdm_ws\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, tdm_addr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tdm_port\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     92\u001b[0m                 password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     93\u001b[0m                 debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Start the connection with the Thymio and variable synchronization.\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m        zeroconf_all - True to use zeroconf with all interfaces instead of default (default: false)\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m     client \u001b[38;5;241m=\u001b[39m \u001b[43mClientAsync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzeroconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzeroconf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzeroconf_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzeroconf_all\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtdm_addr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtdm_addr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtdm_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtdm_port\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtdm_ws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtdm_ws\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpassword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39mwait_for_node(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m node\u001b[38;5;241m.\u001b[39mlock()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tdmclient/clientasync.py:42\u001b[0m, in \u001b[0;36mClientAsync.__init__\u001b[0;34m(self, node_class, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, node_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mClientAsync\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_class \u001b[38;5;241m=\u001b[39m node_class \u001b[38;5;129;01mor\u001b[39;00m tdmclient\u001b[38;5;241m.\u001b[39mClientAsyncCacheNode\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tdmclient/client.py:98\u001b[0m, in \u001b[0;36mClient.__init__\u001b[0;34m(self, zeroconf, zeroconf_all, tdm_ws, tdm_addr, tdm_port, tdm_transport, password, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtdm_transport \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTDM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtdm_addr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtdm_port\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend_handshake(password)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tdmclient/client.py:112\u001b[0m, in \u001b[0;36mClient.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtdm \u001b[38;5;241m=\u001b[39m TDMConnectionWS(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtdm_addr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtdm_ws_port)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtdm \u001b[38;5;241m=\u001b[39m \u001b[43mTDMConnection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtdm_addr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtdm_port\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tdmclient/tcp.py:104\u001b[0m, in \u001b[0;36mTDMConnection.__init__\u001b[0;34m(self, host, port, debug)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39msendall(b)\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mio \u001b[38;5;241m=\u001b[39m \u001b[43mTCPClientIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m=\u001b[39m debug\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tdmclient/tcp.py:96\u001b[0m, in \u001b[0;36mTDMConnection.__init__.<locals>.TCPClientIO.__init__\u001b[0;34m(self, host, port)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, host, port):\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m socket\u001b[38;5;241m.\u001b[39msocket(socket\u001b[38;5;241m.\u001b[39mAF_INET, socket\u001b[38;5;241m.\u001b[39mSOCK_STREAM)\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    await tdmclient.notebook.stop()\n",
    "except:\n",
    "    pass\n",
    "await tdmclient.notebook.start()\n",
    "\n",
    "WALL_THRESHOLD = 500\n",
    "ROBOT_SPEED = 150\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def get_prox():\n",
    "    global prox_horizontal\n",
    "    return prox_horizontal\n",
    "\n",
    "def local_navigation():\n",
    "    global WALL_THRESHOLD, ROBOT_SPEED\n",
    "    max_allowed_speed = 150\n",
    "    set_var(leds_top = [32, 0, 0])\n",
    "    while max(get_prox())>WALL_THRESHOLD:\n",
    "        weight_left = [25,  15, -20, -15, -25]\n",
    "        weight_right = [-25, -15, -15,  15,  25]\n",
    "    \n",
    "        # Scale factors for sensors and constant factor\n",
    "        sensor_scale = 500\n",
    "    \n",
    "        mem_sensor = [0,0,0,0,0]\n",
    "        prox_horizontal = get_prox()\n",
    "        \n",
    "        for i in range(len(mem_sensor)):\n",
    "            # Get and scale inputs\n",
    "            mem_sensor[i] = prox_horizontal[i] // sensor_scale\n",
    "    \n",
    "        y = [ROBOT_SPEED,ROBOT_SPEED]   \n",
    "        \n",
    "        for i in range(len(mem_sensor)):   \n",
    "            # Compute outputs of neurons and set motor powers\n",
    "            y[0] = y[0] + mem_sensor[i] * weight_left[i]\n",
    "            y[1] = y[1] + mem_sensor[i] * weight_right[i]\n",
    "    \n",
    "        # Set motor powers\n",
    "        set_var(motor_left_target = min(y[0],max_allowed_speed))\n",
    "        set_var(motor_right_target = min(y[1],max_allowed_speed))\n",
    "        time.sleep(0.2)\n",
    "    set_var(leds_top = [0, 0, 32])\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "# drive straight with \"speed\" (motor speed)\n",
    "def motors_drive(l_speed=500, r_speed=500):\n",
    "    global motor_left_target, motor_right_target\n",
    "    \n",
    "    motor_left_target = l_speed\n",
    "    motor_right_target = r_speed\n",
    "\n",
    "# drive \"dist\" [cm]\n",
    "def drive(dist=10):\n",
    "    global WALL_THRESHOLD, ROBOT_SPEED\n",
    "    motors_drive(ROBOT_SPEED,ROBOT_SPEED) #test with lower speed value\n",
    "    for i in range(10):\n",
    "        time.sleep((dist/(2*3))/10) #motor speed = 50 equals 2.5cm/s -> calculates how many s you have to drive to cover a distance \"dist\" [cm]\n",
    "        if max(get_prox()) > WALL_THRESHOLD:\n",
    "            print(\"obstacle detected\")\n",
    "            local_navigation()\n",
    "            break\n",
    "    motors_drive(0,0)\n",
    "\n",
    "\n",
    "@tdmclient.notebook.sync_var \n",
    "# turn in a circle with \"speed\" (motor speed)\n",
    "def motors_turn(speed=100):\n",
    "    global motor_left_target, motor_right_target\n",
    "    motor_left_target = -speed\n",
    "    motor_right_target = speed\n",
    "    \n",
    "@tdmclient.notebook.sync_var \n",
    "def motors_turn_right(speed=100):\n",
    "    global motor_left_target, motor_right_target\n",
    "    motor_left_target = speed\n",
    "    motor_right_target = -speed\n",
    "\n",
    "# turn \"angle\" [degrees] anticlockwise\n",
    "def turn_degrees(angle):\n",
    "    if angle < 0:\n",
    "        motors_turn()\n",
    "        time.sleep(abs(angle)*9/360) # 18s for 360° turn with motor speed = 100\n",
    "        motors_turn(speed=0)\n",
    "    else:\n",
    "        motors_turn_right()\n",
    "        time.sleep(abs(angle)*9/360)\n",
    "        motors_turn_right(speed=0)\n",
    "\n",
    "# turn \"angle\" (2. row of Astar_moves) then drive \"dist\" (1.row of Astar_moves)\n",
    "def follow_path(dist,angle):\n",
    "    turn_degrees(angle)\n",
    "    drive(dist)\n",
    "    \n",
    "#used to initialize the follow robot command\n",
    "def follow_path_init(path, current_angle,scale_factor):\n",
    "    Ang = get_angle(path[0],path[1])\n",
    "    Ang = Ang  - current_angle\n",
    "    if abs(Ang) > 180:\n",
    "                    if Ang > 0:\n",
    "                        Ang = Ang-360\n",
    "                    else:\n",
    "                        Ang = Ang+360\n",
    "    dist = get_distance(path[0],path[1])\n",
    "    current_angle = Ang + current_angle\n",
    "    follow_path(dist*scale_factor,-Ang)\n",
    "    return current_angle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f162406-151f-4c88-b5ef-dee8149739cb",
   "metadata": {},
   "source": [
    "### 7.3. Main loop <a class=\"anchor\" id=\"main_loop\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f61d6d4f-304a-4e8d-a3c7-4f054a3c2b91",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Global_nav' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m map_max_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m75\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#Initialize Global path object\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m global_nav \u001b[38;5;241m=\u001b[39m \u001b[43mGlobal_nav\u001b[49m()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#initialize markov filter map \u001b[39;00m\n\u001b[1;32m      7\u001b[0m initialize_maps(map_max_x,map_max_y)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Global_nav' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "map_max_x = 100\n",
    "map_max_y = 75\n",
    "\n",
    "#Initialize Global path object\n",
    "global_nav = Global_nav()\n",
    "#initialize markov filter map \n",
    "initialize_maps(map_max_x,map_max_y)\n",
    "\n",
    "\n",
    "#Set current angle to 0\n",
    "current_angle = 0\n",
    "#robot LED in driving mode\n",
    "set_var(leds_top = [0, 0, 32])\n",
    "\n",
    "i = 0\n",
    "#Initialize camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID') \n",
    "output_ = cv2.VideoWriter('output.avi', fourcc, frames_per_second, (frame_height, frame_width)) \n",
    "while cap.isOpened():\n",
    "    \n",
    "    #image localization\n",
    "    success, frame = cap.read()\n",
    "    if  success:\n",
    "        ## Computer vision detection\n",
    "        array_box_dim_tot, annotated_frame = detection(frame)\n",
    "        \n",
    "        sorted_object = sorted(array_box_dim_tot, key=lambda x: order.get(x[0]))\n",
    "        #break when q is pressed\n",
    "        key = cv2.waitKey(1) & 0xFF \n",
    "        if key == ord('c'):  \n",
    "            break  \n",
    "    else:\n",
    "        print(\"Unable to load the camera pic\")\n",
    "        break\n",
    "\n",
    "    #fixing output from camera, use last output if no object is detected\n",
    "    sorted_object = fix_output(sorted_object,sorted_object_last)\n",
    "    \n",
    "\n",
    "    #blind mode if no object is detected. Follow last calculated path with position and angle update from robot itself\n",
    "    if sorted_object == False: \n",
    "        start = opti_path[1] \n",
    "        opti_path = opti_path[1:]\n",
    "        if len(opti_path) == 1:\n",
    "            print(\"Reached goal\")\n",
    "            set_var(leds_top = [0, 32, 0])\n",
    "            break\n",
    "        current_angle = follow_path_init(opti_path,current_angle,scale_factor)\n",
    "        last_robot_pos = list(start)\n",
    "        #print frame\n",
    "        annotated_frame = frame\n",
    "        cv2.imshow(\"frame\", annotated_frame)\n",
    "        continue #skip rest of loop to try to find object in next frame\n",
    "\n",
    "    #save last output\n",
    "    sorted_object_last = sorted_object\n",
    "\n",
    "    #change format of pic output \n",
    "    start, goal, earth, robot_radius, earth_radius, black_holes_centers, black_holes_radiuss,scale_factor = global_nav.convert_OPENCV_tovalues(sorted_object)\n",
    "    #if distance between robot and mars is less than 5 stop \n",
    "    if get_distance(start,goal) < 4:\n",
    "        print(\"Reached goal\")\n",
    "        set_var(leds_top = [0, 32, 0])\n",
    "        break\n",
    "    \n",
    "    #create map for gloabl navigation\n",
    "    occ, _,_=  global_nav.create_map( goal = goal ,black_holes_centers = black_holes_centers,black_holes_radiuss=black_holes_radiuss)\n",
    "\n",
    "    #use markov filter to update position \n",
    "    if i>0:\n",
    "        start = markov(opti_path[1][0],opti_path[1][1],start[0],start[1],sorted_object[0][2],i==0)        \n",
    "\n",
    "    #return path. Use the opti_path! \n",
    "    opti_path, _, _, _ = global_nav.get_path_straight(start=start,goal=goal)\n",
    "\n",
    "    #convert optipath to format used by camera\n",
    "    scaled_coords = convert_coordinates_2d_list(opti_path)\n",
    "\n",
    "    #annotate frame\n",
    "    if i>0:\n",
    "        annotation(annotated_frame, scaled_coords)\n",
    "    cv2.imshow(\"frame\", annotated_frame)\n",
    "    output_.write(annotated_frame)\n",
    "    \n",
    "    #update current angle. Take the average of the calculated from image and robot motion\n",
    "    if i != 0: \n",
    "        if list(start) != last_robot_pos:\n",
    "            current_angle = update_angle(last_robot_pos,start,current_angle)\n",
    "\n",
    "\n",
    "    #follow first two points in path\n",
    "    current_angle = follow_path_init(opti_path,current_angle,scale_factor)\n",
    "\n",
    "    #save last robot position\n",
    "    last_robot_pos = list(start)\n",
    "\n",
    "    i+=1\n",
    "\n",
    "#print last frame and save video\n",
    "success, frame = cap.read()\n",
    "array_box_dim_tot, annotated_frame = detection(frame)\n",
    "cv2.imshow(\"frame\", annotated_frame)\n",
    "cv2.startWindowThread()\n",
    "cap.release()\n",
    "output_.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c140d721-43dd-44fa-8e58-9a7837db62c0",
   "metadata": {},
   "source": [
    "## 8. Videos Demonstration <a class=\"anchor\" id=\"videos_demonstration\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9bd872-f293-4b28-bc63-de4f018b2c2d",
   "metadata": {},
   "source": [
    "### 8.1. Demonstration without kidnapping  <a class=\"anchor\" id=\"videos_demonstration_without\"></a>\n",
    "<div style=\"text-align: center;\">\n",
    "    <video width=\"640\" height=\"480\" \n",
    "           src=\"./robot_demo.mp4\"  \n",
    "           controls>\n",
    "    </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d165d04c-51e3-4aae-9fac-0b1dc151a8ce",
   "metadata": {},
   "source": [
    "### 8.2. Demonstration with kidnapping <a class=\"anchor\" id=\"videos_demonstration_with\"></a>\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <video width=\"640\" height=\"480\" \n",
    "           src=\"./robot_demov1.mp4\"  \n",
    "           controls>\n",
    "    </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf60694-81d6-4ca5-ab9d-3f251c96622d",
   "metadata": {},
   "source": [
    "### 9. Conclusion <a class=\"anchor\" id=\"conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe87c41-3d43-4e52-8397-8e37561af098",
   "metadata": {},
   "source": [
    "In space a robust system is important. That is why we chose to go with a system that updates all postitions and paths in each step. While this is not a fast method, since posistions and global path has to be recomputated, the method allows all part of the system to move and still have the space ship (robot) find its way to Mars. Doing the navigation in steps meant that we could not achive a smooth trajectory of the robot. However, this method did prove to make the whole system very robust. We can move the camera, move the robot, and move obstacles and still have the space ship land on Mars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1887d726-109a-43a4-affe-3e162e699697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
