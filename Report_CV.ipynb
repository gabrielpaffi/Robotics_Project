{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62af446a-37e2-4a86-9bfe-44806f37f2fc",
   "metadata": {},
   "source": [
    "# Computer vision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1008761a-e007-452a-9a8a-1db2a1946624",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [1. Yolo v8 Introduction](#chapter1)\n",
    "* [2. Application of Yolov8 on the thymio](#chapter2)\n",
    "    * [2.1 Images of System Detection](#section_2_1)  \n",
    "* [3. Detection function](#chapter3)\n",
    "  * [3.1 Packages required](#section_3_1)\n",
    "  * [3.2 Explanation of the function](#section_3_2)\n",
    "* [4. Annotation function](#chapter4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feb869b-984e-43ee-ae74-0eb2f1817107",
   "metadata": {},
   "source": [
    "## 1. YOLO v8 Introduction <a class=\"anchor\" id=\"chapter1\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008b8e6f-642c-406f-9c1d-707af8d485cf",
   "metadata": {},
   "source": [
    "YOLOv8 is the newest state-of-the-art YOLO model that can be used for object detection, image classification, and instance segmentation tasks. YOLOv8 was developed by Ultralytics, who also created the influential and industry-defining YOLOv5 model. YOLOv8 includes numerous architectural and developer experience changes and improvements over YOLOv5.\n",
    "\n",
    "Yolov8 is based on a Convolutional neural network, the main idea is to train a model using this CNN with an adapted dataset.\n",
    "\n",
    "\n",
    "Convolutional neural network (CNN) is a regularized type of feed-forward neural network that learns feature engineering by itself via filters (or kernel) optimization. Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by using regularized weights over fewer connections.For example, for each neuron in the fully-connected layer 10,000 weights would be required for processing an image sized 100 Ã— 100 pixels.\n",
    "\n",
    "Sources :\n",
    "\n",
    "https://blog.roboflow.com/whats-new-in-yolov8/\n",
    "\n",
    "https://en.wikipedia.org/wiki/Convolutional_neural_network\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e959b2a5-eddd-417b-80c8-d12257d7bb9a",
   "metadata": {},
   "source": [
    "## 2. Application of Yolov8 on the thymio <a class=\"anchor\" id=\"chapter2\"></a>\n",
    "\n",
    "\n",
    "The basic idea was to use yolov8 to detect the robot, start point and obstacles at any point in time. We first train our model with :\n",
    "- annotation of a dataset on Robotflow (https://app.roboflow.com/gabriel-paffi)\n",
    "- training our system on a google colab to accelare our training process (https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-object-detection-on-custom-dataset.ipynb#scrollTo=I4bpUIibcV1l)\n",
    "\n",
    "Once we have obtained our model trained on our dataset, we can use this model to predict the class, position and confidence of any object of our datasets.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4966d286-08ee-4924-b0b3-1b99b658a87a",
   "metadata": {},
   "source": [
    "### 2.1 Images of System Detection <a class=\"anchor\" id=\"section_2_1\"></a>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./cv_images/robot.png\" alt=\"System detection objects\">\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./cv_images/System.png\" alt=\"System detection objects\">\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9605f0e-3c8c-45f0-b347-c665f0f1bfdb",
   "metadata": {},
   "source": [
    "## 3. Detection function <a class=\"anchor\" id=\"chapter3\"></a>\n",
    "\n",
    "\n",
    "### 3.1 Packages required <a class=\"anchor\" id=\"section_3_1\"></a>\n",
    "\n",
    "- open cv2 : https://opencv.org/\n",
    "- Yolov8 from ultralytics : https://github.com/ultralytics/ultralytics\n",
    "\n",
    "### 3.2 Explanation of the function <a class=\"anchor\" id=\"section_3_2\"></a>\n",
    "\n",
    "This function take into input a frame and return :\n",
    "- an array containing the different object as follow : (class, x_center, y_center, width_object, height_object)\n",
    "- a frame annotated with the class object on it and the confidence accord to the object\n",
    "\n",
    "This function use our train model (robotv12.pt) and a minimum confidence to detect an object of 0.55 (1 is maximum confidence).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb0162c9-af6f-41b0-8932-e8bd96d6b4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "model = YOLO(\"robotv12.pt\")\n",
    "class_object = [\"blackholes\",\"earth\",\"mars\", \"robot\" ]\n",
    "\n",
    "def detection(frame):\n",
    "\n",
    "\n",
    "\n",
    "    results = model.track(frame, conf = 0.55, verbose = False)\n",
    "\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "\n",
    "    array_box_dim_tot = []\n",
    "           \n",
    "   \n",
    "\n",
    "    for objects in results:\n",
    "    \n",
    "            number_objects = len(objects.boxes.cls)\n",
    "            box_dimensions = objects.boxes.xywhn # normalised dimensions to frame (value of x,y, width and height between 0 and 1)\n",
    "            for i in range(number_objects) :\n",
    "              \n",
    "                array_box_dim_object = []\n",
    "                confidence = round(objects.boxes.conf[i].item(),3)\n",
    "                cls = objects.boxes.cls[i].item()\n",
    "                array_box_dim = box_dimensions[i].tolist() # center x,y width, height\n",
    "                for i in range(len(array_box_dim)):\n",
    "                    array_box_dim[i] = round(array_box_dim[i],2)\n",
    "                array_box_dim_object = [class_object[int(cls)], array_box_dim, confidence]\n",
    "\n",
    "                array_box_dim_tot.append(array_box_dim_object)\n",
    "\n",
    "\n",
    "                        \n",
    "\n",
    "    return(array_box_dim_tot, annotated_frame)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c70298-5897-4270-a4ed-4c7aed48ed65",
   "metadata": {},
   "source": [
    "## 4. Annotation function <a class=\"anchor\" id=\"chapter4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa68a5a-2e2a-497b-86a5-ebad022c1464",
   "metadata": {},
   "source": [
    "This function goal is to add the path of the robot on the frame.\n",
    "The input of this function are :\n",
    "- a frame\n",
    "- coordinates : the coordinates of each point from the robot towards the goal\n",
    "\n",
    "The output is the frame with segment between each point of the path.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90130b2e-668d-4919-a174-cffd71600171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation(frame, coord):\n",
    "    \n",
    "    for k in range(len(coord)-1):\n",
    "        \n",
    "        \n",
    "        frame = cv2.circle(frame, coord[k], circle_radius, blue,circle_thicknes)\n",
    "        frame = cv2.line(frame, coord[k], coord[k+1], blue, line_thickness)  \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f46c42-7f91-46ec-9e35-ea1403c95aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
